# DP-203T00: Data Engineering in Azure

「DP-203: Data Engineering on Azure」コースへようこそ。このコースをサポートするため、コースの内容を更新して、コースで使用される Azure サービスを最新の状態に維持する必要があります。  ラボの手順とラボ ファイルは GitHub で公開しています。これにより、コースの作成者と MCT 間でのオープンな作業が可能となり、Azure プラットフォームの変更に合わせてコンテンツを最新の状態に保つことができます。

## ラボの概要

各モジュールの課題の目的を以下に概説します。

### 1 日目

#### [モジュール 00: ラボ環境のセットアップ](Instructions/Labs/LAB_00_lab_setup_instructions.md)

このコースのラボ環境のセットアップを完了します。

#### [モジュール 01: データ エンジニアリング ワークロードのコンピューティングおよびストレージ オプションを確認する](Instructions/Labs/LAB_01_compute_and_storage_options.md)

このラボでは、データ レイクを構成し、探索、ストリーミング、バッチ ワークロードに備えてファイルを最適化する方法を説明します。受講者は、バッチおよびストリーム処理を通してファイルを変換しながら、データを絞り込めるレベルにデータ レイクを整理する方法を学びます。また、Azure Synapse Analytics で Apache Spark を使用する経験も積みます。  データセットで CSV、JSON、Parquet ファイルのようなインデックスを作成し、これを使用して Hyperspace や MSSParkUtils などの Spark ライブラリでクエリやワークロード アクセラレーションを行う方法を学びます。

#### [モジュール 02: Azure Synapse Analytics サーバーレス SQL プールを使用してインタラクティブなクエリを実行する](Instructions/Labs/LAB_02_queries_using_serverless_sql_pools.md)

このラボでは、Azure Synapse Analytics のサーバーレス SQL プールで T-SQL ステートメントを実行し、データ レイクと外部ファイル ソースに格納されているファイルを使用する方法を学びます。データ レイクに格納されている Parquet ファイルと、外部データ ストアに格納されている CSV ファイルのクエリを実行します。次に、Azure Active Directory セキュリティ グループを作成し、ロールベースのアクセス制御 (RBAC) とアクセス制御リスト (ACL) を使用してデータ レイクのファイルにアクセスします。

#### [モジュール 03: Azure Databricks でのデータの探索と変換](Instructions/Labs/LAB_03_data_transformation_in_databricks.md)

このラボでは、さまざまな Apache Spark DataFrame メソッドを使用して、Azure Databricks でデータを探索して変換する方法を説明します。受講者は、標準的な DataFrame メソッドを実行してデータの探索と変換を行う方法を学びます。また、重複データの削除や、日時の値の操作、列の名前変更、データの集計など、より高度なタスクを実行する方法を学習します。選択した取り込み技術をプロビジョニングし、これを Stream Analytics と統合して、ストリーミング データで動作するソリューションを作成します。

### 2 日目

#### [モジュール 04: Apache Spark を使用してデータの探索と変換を行い、データ ウェアハウスに読み込む](Instructions/Labs/LAB_04_data_warehouse_using_apache_spark.md)

このラボでは、データ レイクに格納されているデータを探索し、データを変換して、リレーショナル データ ストアにデータを読み込む方法を学びます。Parquet ファイルと JSON ファイルを探索し、階層構造を使用して JSON ファイルのクエリと変換を実行する技術を使用します。その後、Apache Spark を使用してデータをデータ ウェアハウスに読み込み、データ レイクの Parquet データを専用 SQL プールのデータに統合します。

#### [モジュール 05: データ ウェアハウスにデータを取り込んで読み込む](Instructions/Labs/LAB_05_load_data_into_the_data_warehouse.md)

このラボでは、T-SQL スクリプトと Synapse Analytics 統合パイプラインを介してデータ ウェアハウスにデータを取り込む方法を説明します。受講者は、T-SQL を使用して PolyBase と COPY で Synapse 専用 Synapse SQLプールにデータを読み込む方法を学びます。また、ペタバイト スケールのデータ インジェスト向けに Azure Synapse パイプラインでコピー アクティビティとともにワークロード管理を使用する方法も学習します。

#### [モジュール 06: Azure Data Factory または Azure Synapse パイプラインでデータを変換する](Instructions/Labs/LAB_06_transform_data_with_pipelines.md)

このラボでは、データ統合パイプラインを構築して、複数のデータ ソースから取り込み、マッピング データ フローとノートブックを使用してデータを変換し、ひとつ以上のデータシンクにデータを移動する方法を説明します。

### 3 日目

#### [モジュール 07: ノートブックのデータを Azure Data Factory または Azure Synapse パイプラインと統合する](Instructions/Labs/LAB_07_integrate_data_from_notebooks.md)

このラボでは、過去 12 ヶ月間のユーザーのアクティビティと購入を照会するためにノートブックを作成します。その後、新しいノートブック アクティビティを使用してノートブックをパイプラインに追加し、調整プロセスの一環としてマッピング データ フローの後でこのノートブックを実行します。これを構成する間に、制御フローでダイナミック コンテンツを追加し、どのようにパラメーターを使用できるのか検証します。

#### [モジュール 08: Azure Synapse Analytics を使用したエンドツーエンドのセキュリティ](Instructions/Labs/LAB_08_security_with_synapse_analytics.md)

このラボでは、Synapse Analytics ワークスペースとその補助インフラストラクチャを保護する方法を学習します。SQL Active Directory Admin の観察、IP ファイアウォール ルールの管理、Azure Key Vault を使用したシークレットの管理、Key Vault にリンクされたサービスとパイプライン アクティビティによるシークレットへのアクセスを実行します。専用 SQL プールを使用する際の列レベルのセキュリティ、行レベルのセキュリティ、動的データ マスクの実装方法を学びます。

#### [モジュール 09: Azure Synapse Link を使用してハイブリッド トランザクション分析処理 (HTAP) に対応する](Instructions/Labs/LAB_09_htap_with_azure_synapse_link.md)

このラボでは、Azure Synapse Link によって Azure Cosmos DB アカウントを Synapse ワークスペースにシームレスに接続する方法を学習します。Synapse Link を有効にして構成する方法、および Apache Spark プールと SQL サーバーレス プールを使用して Azure Cosmos DB 分析ストアのクエリを行う方法を学びます。
### 4 日目
#### [モジュール 10: Stream Analytics によるリアルタイムのストリーム処理](Instructions/Labs/LAB_10_stream_analytics.md)

このラボでは、Azure Stream Analytics を使用してストリーミング データを処理する方法を学習します。車両のテレメトリ データを Event Hubs に取り込んだ後、Azure Stream Analytics のさまざまなウィンドウ化関数を使用してリアルタイムでそのデータを処理します。データは Azure Synapse Analytics に出力されます。最後に、スループットを増やすために Stream Analytics ジョブのスケーリングを行う方法を学びます。

#### [モジュール 11: Event Hubs と Azure Databricks を使用してストリーム処理ソリューションを作成する](Instructions/Labs/LAB_11_stream_with_azure_databricks.md)

このラボでは、Azure Databricks で Event Hubs と Spark Structured Streaming を使用して大規模なストリーミング データの取り込みと処理を行う方法を学習します。構造化ストリーミングの主な機能と使用方法について学びます。スライディング ウィンドウを実装して、データのチャンクで集計を行い、基準値を適用して古いデータを削除します。最後に、Event Hubs に接続して、ストリームの読み取りと書き込みを行います。

- **MCT ですか? ** - [MCT 向け GitHub ユーザー ガイド](https://microsoftlearning.github.io/MCT-User-Guide/)をご覧ください。
                                                                       
## 公開済みの MOC ファイルと一緒にこれらのファイルを使用する方法

- 講師用ハンドブックと PowerPoint が、コースの内容を教えるための主要なソースであることに変わりはありません。

- GitHub にあるファイルは、受講者ハンドブックと組み合わせて使えるように設計されています。ただし、MCT とコースの作成者が最新のラボ ファイルのソースを共有できるように、中央リポジトリとして GitHub に置いてあります。

- 各モジュールのラボの手順は /Instructions/Labs フォルダーに含まれています。このフォルダー内の各サブフォルダーは各モジュールを参照しています。たとえば、Lab01 は module01 に関係があります。ラボの手順の各フォルダーには、受講者が従うことのできる README.md ファイルがあります。

- 講師は資料の配信時に、最新の Azure サービスをサポートするために行われた変更がないか GitHub を確認し、配信用の最新ファイルを取得することをお勧めします。

- ラボの手順に掲載されている画像の中には、このコースで使用するラボの環境の状態を必ずしも反映していないものもあります。たとえば、データ レイクでファイルを参照する際、実際の環境では存在しない追加フォルダーが画像に表示されている可能性があります。これは意図的なもので、ラボの手順には影響しません。

## 受講者用ハンドブックの変更について

- 受講者用ハンドブックは四半期ごとに確認が行われ、必要に応じて通常の MOC リリース チャンネルを通じて更新されます。

## 貢献するには?

- MCT は、GitHub repro のコードまたはコンテンツに問題を送信できます。Microsoft とコース作成者は、必要に応じてコンテンツとラボのコード変更をトリアージして含めます。

## 教材

MCT およびパートナーがこれらの資料にアクセスし、順番に受講生に個別に提供することを強くお勧めします。  受講生が GitHub に直接アクセスして、進行中のクラスの一部としてラボの手順にアクセスすると、コースの一部として別の UI にアクセスしなければならず、受講生にとっては混乱を招くことになります。受講生が個別のラボの指示を受ける理由に関する説明から、常に変化するクラウドベースのインターフェイスとプラットフォームの性質がよく分かります。GitHub 上のファイルにアクセスするための Microsoft Learning サポートと GitHub サイトのナビゲーションのサポートは、このコースのみを教える MCT に限定されます。

## 演習内容

- このコースをサポートするために、コースで使用される Azure サービスを最新の状態に保つために、コース コンテンツを頻繁に更新する必要があります。  ラボの手順とラボ ファイルは GitHub で公開しています。これにより、コースの作成者と MCT 間でのオープンな作業が可能となり、Azure プラットフォームの変更に合わせてコンテンツを最新の状態に保つことができます。

- ラボでの共同作業をこれまで以上に向上できることを願っています。Azure サービスの内容が変更され、修正が必要な点をラボの実施中に見つけた場合は、ラボのソースをすぐに修正してください。  MCT をお互い支援できるようご協力お願いします。

## リリース済みの MOC ファイルと比べ、これらのファイルはどのように利用できますか?

- 講師用ハンドブックと PowerPoint が、コースの内容を教えるための主要なソースであることに変わりはありません。

- GitHub にあるファイルは、受講者ハンドブックと組み合わせて使えるように設計されています。ただし、MCT とコースの作成者が最新のラボ ファイルのソースを共有できるように、中央リポジトリとして GitHub に置いてあります。

- 講師は、ラボを行うたびに、最新の Azure サービスに合わせて修正された個所がないか GitHub を確認し、最新のラボ用ファイルを取得してください。

## 受講者ハンドブックの変更については?

- 受講者用ハンドブックは四半期ごとに確認が行われ、必要に応じて通常の MOC リリース チャンネルを通じて更新されます。

## 貢献するには?

- MCT は、GitHub repro のコードまたはコンテンツにプル要求を送信できます。Microsoft とコースの作成者は、必要に応じてコンテンツとラボ コードの変更をトリアージして含めます。

- バグ、変更、改善、アイデアを提出できます。  新しい Azure 機能を先に見つけたら、  新しいデモを送信してください。

## 注

### コース資料

MCT およびパートナーがこれらの資料にアクセスし、順番に受講生に個別に提供することを強くお勧めします。  コースを開催中にラボの手順を示す目的で、受講者に直接 GitHub へアクセスするよう指示した場合、コースの中で別の UI を扱わなければならず、受講者にとっては混乱を招くことになります。ラボのたびに別の教材を提供する理由として、クラウドベースのインターフェイスやプラットフォームは常に進化し続けているという特質を強調して伝えられます。GitHub 上のファイルへのアクセスと GitHub サイトのナビゲーションに関する Microsoft Learning のサポートは、このコースを指導する MCT のみが利用できます。
